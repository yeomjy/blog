---
title: Discrete Differential Geometry and Graph Convolution
date: 2022-12-30 11:30:00 +0900
categories: [Machine Learning, Geometric Deep Learning]
tags: [Machine Learning, Graph Neural Network, Differential Geometry, Differential Form, Geometric Deep Learning, Discrete Fourier Transform, Discrete Differential Geometry]
pin: false
math: true
author: yeomjy
---

> This post requires some basic knowledge of manifold, differential form, simplicial complex, homology, and cohomology.
> If you don't have experience about them,
> please see my introductory posts on [differential geometry](/blog/posts/diff-geo-1) and [algebraic topology](/blog/posts/alg-top-1)
> or some books in reference.
{: .prompt-tip}

# Geometric Deep Learning and Graph Neural Network

Deep Learning is one of the most greatest advance in machine learning.
But, mathematically, neural network is just a parameterized function
$\renewcommand{\R}{\mathbb{R}}f_{\theta}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and there is no way to
enforce geometric structure of its domain to neural network.

For example, let us assume that our domain is graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
Typically in graph machine learning, there is a node feature vector $X_v \in \mathbb{R}^d$ for each $v \in \mathcal{V}$.

Then we can make a neural network which gets input
$X = \left[X_{v_1}, X_{v_2}, \cdots, X_{v_{|\mathcal{V}|}}\right] \in \mathbb{R}^{|\mathcal{V}| \times d}$, and adjacency matrix $A$.

Since the ordering of graph does not have meaning,

$$
    f_{\theta} \left(X_{v_{\sigma(1)}}, \cdots, X_{v_{\sigma(|\mathcal{V}|)}}, PAP^T\right) = f_{\theta} \left(X_{v_1}, \cdots, X_{v_{|\mathcal{V}|}}, A\right) 
$$

should hold for every permutation $\sigma$ and corresponding permutation matrix $P$ (this property is called permuation invariance), or

$$
    f_{\theta} \left(X_{v_{\sigma(1)}}, \cdots, X_{v_{\sigma(|\mathcal{V}|)}}, PAP^T\right) = Pf_{\theta} \left(X_{v_1}, \cdots, X_{v_{|\mathcal{V}|}}, A\right)  
$$

should hold(this property is called permutation equivariance).

But there is no way to enforce these structures to $f_{\theta}$.

Geometric Deep Learning is a research field of deep learning, which studies to construct
neural network with geometric 'symmetry' of domain.
The major domains of geometric deep learning is
Grids, Groups, Graphs, Geodesics, and Gauges.

Ordinary convolutional layer is a well-known instance of geometric deep learning.
It is 'equivariant' layer of 'Grid'.

For 'Graph', we want to make a permutation equivariant layer with the ordering of graph.
The discrete differential geometry can propose an answer to this question, which we call
(spectral) graph convolution.

# Discrete Differential Geometry

## Laplacian

For $f: \R^n \rightarrow \R$, we defined laplacian operator

$$
    \Delta f = \sum_{i=1}^{n} \frac{\partial^2 f}{\partial x_i^2}
$$

Now we can generalize this operator to any $k$-form, which called **Laplace-Beltrami operator** or **Laplace-de Rham operator**.

### Hodge Star

### Co-differential

### Generalization of Laplacian operator


**Laplace-Beltrami operator** 

**Laplace-De Rham operator**

## Discrete Exterior Calculus

*TODO*

### Abstract Simplicial Complex

### Discrete Exterior Calculs

### Combinatorial Laplacian

# Graph Convolutional Network(GCN)

Now it's time to talk about *Graph Convolution*.

First, we can think node feature vectors

$$
    X = \left[X_{v_1}, X_{v_2}, \cdots, X_{v_{|\mathcal{V}|}}\right] \in \mathbb{R}^{|\mathcal{V}| \times d}
$$

is stack of $d$ functions defined on $\mathcal{V}$.

That is, to think $(X_{v_i})_j = f_j(v_i)$,
for each $f_j: \mathcal{V} \rightarrow \R \simeq \R^{|\mathcal{V}|}$

Then, it is sufficient to define a convolution operator to each $f_j$, because we can stack multiple
convolutional layer to process each *channel* of input, like convolutional layer for image.

## Convolution

Let $\mathcal{F}$ be a fourier trasnform operator, and $ \star $ be a convolution operator.
Then

$$\mathcal{F}(f \star g) = \mathcal{F}(f) \mathcal{F}(g)$$

holds.
With this relationship, let $f, g \in \R^{|\mathcal{V}|} $ be functions defined on $\mathcal{V}$.
 we will define **graph convolution** by

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
$$

for appropriate *fourier transform* operator for functions on $\mathcal{V}$.
Where $\odot$ represents elementwise multiplication.

## Discrete Fourier Transform and Graph Fourier Transform

First, let us define **discrete fourier transform(DFT)**.

Let $(f_0, \cdots f_{d-1}) \in \R^d$ be a discrete signal(discrete function values), then
the discrete fourier transform is

$$
    s_k = \frac{1}{\sqrt{d}} \sum_{t=0}^{d-1} f_t e^{i \frac{2\pi k}{d} t}
$$

It is just a natural discretization of fourier transform.

Then, by straightforward calculation, $ \Delta_{t} e^{i \frac{2\pi k}{d} t} = \frac{d^2}{dt^2} e^{i \frac{2\pi k}{d} t} = - \left(\frac{2\pi k}{d}\right)^2e^{i \frac{2\pi k}{d}t}$.

It means that eigenvector of laplacian is fourier basis.

Since we seen that graph laplacian is laplacian operator in the context of discrete exterior calculus,
we define **graph fourier transform** by the same way,

___

**Definition (Graph Fourier Transform).**

___

The graph fourier transform for function on graph
$f \in \R^{|\mathcal{V}|}$ is

$$
    s = U^T f
$$

where $U = [u_1, \cdots, u_{|\mathcal{V}|}]$ are (normalized) eigenvectors of graph laplacian
$L = D - A$.

___

## Graph Convolution

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph.
Let $f: \mathcal{V} \rightarrow \mathbb{R}$ be a function defined on vertex set.

___

**Definition (Graph Convolution Layer).**

___

The graph convolution layer is defined as

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
    = U (U^T f \odot U^T g) = U \textrm{diag} (U^T g) U^T f
$$

___

# Remarks

1. Graph convolution methods using graph laplacian are called *spectral graph convolution*. Other methods, using message passing framework to construct graph convolution are called *spatial graph convolution*.
2. The *Diffusion Kernel*, which is kernel on feature vector of vertex[2], also can be interpreted with
discrete differential geometry.
The *Diffusion Equation* $  \partial_t f = \mu \Delta f$ with graph laplacian gives the reason why we call it 'Diffusion'.
More specifically, the kernel element $K_{ij}$ can be interpreted as probability of
$i \rightarrow j$ at continuous random walk, and the random walk can be interpreted as
diffusion process on graph along edge.

# References

[1] Do Carmo, M. P. *Differential Forms and Applications*. Springer Science & Business Media, 1998.  
[2] Kondor, R. I. and Lafferty, J. Diffusion kernels on graphs and other discrete structures. *In proceedings of th 19th international conference on machine learning*, volume 2002, pp. 315-322, 2022.  
[3] Lee, J. M. *Introduction to Smooth Manifolds*, Springer, 2012.  
[4] Hamilton, W. L. Graph representation learning, *Synthesis Lectures on Artifical Intelligence and Machine Learning*, 14(3):1-159, 2020  
[5] Spivak, M. *Calculus on manifolds: a modern approach to classical theorems of advanced calculus*. CRC press, 2018.  
[6] McInerney, A. *First Steps in Differential Geometry*. Springer-Verlag, 2013  
[7] Crane, K. Discrete differential geometry: An applied introduction, *Notices of the AMS, Communication*, pp.1153-1159, 2018.  
[8] Lee, J. M. *Introduction to topological manifolds*, volume 202. Springer Science & Business Media, 2010.  
[9] Bronstein, M. M., Bruna, J., Cohen, T., and Velickovic, P. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. *arXiv preprint arXiv:2104.13478*, 2021
