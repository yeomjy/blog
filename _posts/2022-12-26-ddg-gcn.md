---
title: Discrete Differential Geometry and Graph Convolution
date: 2022-12-26 11:15:00 +0900
categories: [Machine Learning, Graph Neural Network]
tags: [Machine Learning, Graph Neural Network, Differential Geometry, Geometric Deep Learning, Discrete Fourier Transform]
pin: false
math: true
---

## Geometric Deep Learning and Graph Neural Network

Deep Learning is one of the most greatest advance in machine learning.
But, mathematically, neural network is just a parameterized function
$f_{\theta}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and there is no way to
enforce geometric structure of its domain to neural network.

For example, let us assume that our domain is graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
Typically in graph machine learning, there is a node feature vector $X_v \in \mathbb{R}^d$ for each $v \in \mathcal{V}$.

Then we can make a neural network which gets input
$X = \left[X_{v_1}, X_{v_2}, \cdots, X_{v_{|\mathcal{V}|}}\right] \in \mathbb{R}^{|\mathcal{V}| \times d}$, and adjacency matrix $A$.

Since the ordering of graph does not have meaning,

$$
    f_{\theta} \left(X_{v_1}, \cdots, X_{v_{|\mathcal{V}|}}, A\right) = f_{\theta} \left(X_{v_{\sigma(1)}}, \cdots, X_{v_{\sigma(|\mathcal{V}|)}}, PAP^T\right)
$$

should hold for every permutation $\sigma$ and corresponding permutation matrix $P$ (this property is called permuation invariance).
But there is no way to enforce permutation invariance to $f_{\theta}$.

Geometric Deep Learning is a research field of deep learning, which studies to construct
neural network with geometric 'symmetry' of domain.
The major domains of geometric deep learning is
Grids, Groups, Graphs, Geodesics, and Gauges.

Ordinary convolutional layer is a well-known instance of geometric deep learning.
It is 'equivariant' layer of 'Grid'.

For 'Graph', we want to make a permutation invariance layer with the ordering of graph.
The discrete differential geometry can propose an answer to this question, which we call
(spectral) graph convolution.

## Discrete Differential Geometry

### A Quick Introduction to Differential Geometry

Let us first review basic notions of differential geometry.$\renewcommand{\R}{\mathbb{R}}$

#### Topological manifold

Topological manifold $M$ is a topological space such that locally euclidean, hausdorff, second-countable.
The word 'locally euclidean' means that for each $p \in M$, there exists an open neighborhood
$p \in U \subset M$ such that there exists a homeomorphism $\psi: U \rightarrow V$,
where $V$ is an open subset of $\R^n$.
We call the pair $(U, \psi)$ a *coordinate chart*.

##### Manifold with boundary

#### Smooth manifold

Smooth manifold $M$ is a topological manifold which every two coordinate charts
$(U, \psi), (U', \phi)$ satisfies if $U \cap U' \neq \emptyset$, then

$$
\phi \circ \psi^{-1} : \psi( U \cap U' )\rightarrow \phi(U \cap U'), \psi \circ \phi^{-1} : \phi(U \cap U') \rightarrow \psi(U\cap U')
$$

are smooth function on $\R^n$.
![manifold](https://ncatlab.org/nlab/files/ChartsOfAManifold.png)
*Image from: [nlab](https://ncatlab.org/nlab/show/differentiable+manifold)*
Now we can define differentiable map by local coordinate.
$f: M \rightarrow \R^k$ is differentiable if for each $p \in M$, there is a coordinate chart
$(U, \psi)$ containing $p$ such that
$f \circ \psi^{-1}: \psi(U)\subset \R^n \rightarrow \R^k$ is differentiable.
Moreover, if $M, N$ are differentiable manifolds, then
$f: M \rightarrow N$ is differentiable if for each $p \in M$ there are coordinate chart
$(U, \psi)$ containing $p$ and $(V, \varphi)$ containing $f(p)$ such that
$\varphi \circ f \circ \psi^{-1}: \psi(U) \rightarrow \varphi(V)$ is differentiable.

#### Tangent vector and tangent space

In calculus and undergraduate differential geometry, we dealt with manifold as an locally euclidean set which embedded in some other euclidean space.
We defined a tangent vector at $p$ by $\alpha'(0)$, where $\alpha: I \rightarrow M$ is a
smooth curve and $\alpha(0) = p$, and the tangent space $T_p M$ is a vector space of all tangent vectors at $p$.
And for $f: M \rightarrow N$, we defined a differential of $f$ at $p$, $df_p: T_pM \rightarrow T_{f(p)}N$

$$
    df_p (v) = (f \circ \alpha)'(0)
$$

where $\alpha(0) = p, \alpha'(0) = v$.

Let $f: M \rightarrow \R$, then we defined directional derivative

$$
D_v f(p) = \lim_{t \rightarrow 0}\frac{f(p + tv) - f(p)}{t} = \left.\frac{d}{dt}\right|_{t=0} f(p + tv) = \sum_{i=1}^{n} v_i \frac{\partial f}{\partial x_i}(p) = \nabla f(p) \cdot v = df_p(v)
$$  

Now, in abstract manifold setting, we should use local coordinates.
Let $p \in M$, $(U, \psi)$ be a coordinate chart containing $p$, $\alpha: I \rightarrow M$ be a differentiable curve, and $\psi(p) = (x_1(p), \cdots, x_n(p)) = x_0$
Then

$$
D_v f(p) = \left.\frac{d}{dt} f(\alpha(t))\right|_{t=0} = \left.((f\circ \psi^{-1}) \circ (\psi \circ \alpha))\right|_{t=0} = \sum_{i=1}^{n} \left. \frac{\partial (f \circ \psi^{-1})}{\partial x_i} \right|_{x=x_0} \left.\frac{d}{dt} x_i(\alpha(t)) \right|_{t=0}
$$

If $\psi(\alpha(t)) = (x_1, \cdots, x_i + t, \cdots, x_n) $, then we can get *partial derivative*

$$
    \frac{\partial f}{\partial x_i} = \frac{\partial (f \circ \psi^{-1})}{\partial x_i}
$$

Now let us assume that we define *tangent vector* as $(\psi \circ \alpha)'(0)$, then this definition
depends to the coordinate chart $\psi$, so we need independent definition to $\psi$.

Since for each $f, v$ we can define $df_p(v)$, we can think $v$ as a map from $\mathcal{C}^{\infty}(M)$ such that $f \mapsto df_p(v)$.

Therefore, we define *tangent vector* $v_p$ at $p$ as a map

$$ v_p:\mathcal{C}^{\infty}(M) \rightarrow \R $$

$$v_p(af + bg) = a v_p(f) + b v_p(g)$$

$$v_p(fg) = v_p(f)g(p) + f(p)v_p(g)$$

We call it a *derivation*.

Of course, for each $v \in T_p M$, given coordinate chart $(U, \psi)$, there exists $v_1, \cdots, v_n$ such that

$$
    v = \sum_{i=1}^{n} v_i \left.\frac{\partial}{\partial x_i}\right|_{p}
$$

where
$$
\left.\frac{\partial}{\partial x_i}\right|_{p} (f) = \left.\frac{\partial (f \circ \psi^{-1})}{\partial x_i}\right|_{\psi(p)}
$$

#### Vector field

Vector field $X$ on $M$ is an assignment of tangent vectors $T_p M$ for each $p \in M$.
i.e.

$$
    X: M \rightarrow TM = \bigcup_{p \in M} \{p\} \times T_p M
$$

and we denote

$$
    X(p) = \sum_{i} v_i (p) \left. \frac{\partial}{\partial x_i} \right|_{p}
$$

where $\left. \frac{\partial}{\partial x_i} \right|_{p}$ are standard basis of $T_p M$.
We say $X$ is smooth if $v_i(p)$ are smooth with respect to $p$.

### A Quick Introduction to Differential Forms

#### Tensor and Tensor product

Let $V_1, \cdots, V_p, W_1, \cdots, W_q$ be vector spaces, then
$(p, q)$-tensor $T$ is a multilinear functional

$$
    T: V_1^{*} \times \cdots \times V^{*}_p \times W_1 \times \cdots \times W_q \rightarrow \R
$$

In the context of differential form, we only consider $(0, k)$-tensor (or simply, $k$-tensor)

$$
    T: V\times \cdots \times V \rightarrow \R
$$

and we denote the space of $k$-tensor as $\mathfrak{T}^{k}(V)$.

We can define a tensor product $\otimes$ which gets $p$-tensor and $q$-tensor and outputs $(p+q)$-tensor by

$$
    (T \otimes S) (v_1,\cdots, v_p, v_{p+1}, \cdots, v_{p+q}) = T(v_1,\cdots,v_p) S(v_{p+1}, \cdots, v_{p+q})
$$

We can write tensor as a form of basis expansion

$$
    T = \sum_{i_1, \cdots, i_{k}} T_{i_1, \cdots, i_k} dx_{i_1} \otimes \cdots \otimes dx_{i_k}
$$

where $T_{i_1, \cdots, i_k} \in \R$, $dx_i$ are dual basis of $V$.

#### Alternating tensor and Wedge product

$k$-alternating form $T$ is a $k$-tensor such that

$$ T(\cdots, v_i, \cdots, v_j, \cdots) = -T(\cdots, v_j , \cdots, v_i, \cdots)$$

and we denote the space of all alternating $k$-tensor as $\Lambda^{k}(V)$.

We also want to write $\omega \in \Lambda^{k}(V)$ as a form of basis expansion.
Therefore, we define *wedge product* $\wedge$ by

$$
    (\phi_1 \wedge \cdots \wedge \phi_k)(v_1, \cdots, v_k) = \det \left( \begin{array}{ccc}
        \phi_1 (v_1)  & \cdots & \phi_1(v_k) \\
        \vdots & \ddots & \vdots \\
        \phi_k (v_k) & \cdots & \phi_k(v_k)
     \end{array} \right)
$$

Where $\phi_i$ are $1$-form.

Then we can write $\omega \in \Lambda^{k}(V)$ by

$$ \omega = \sum_{i_1< \cdots< i_k} \omega_{i_1 , \cdots , i_k} dx_{i_1} \wedge \cdots \wedge dx_{i_k} $$

and if $\omega = \sum_{I} \omega_I dx_I, \varphi = \sum_{J} \varphi_J dx_J$, then

$$
    \omega \wedge \varphi = \sum_{IJ} \omega_I \varphi_J dx_I \wedge dx_J
$$

#### Differential forms

Differential $k$-form on manifold $M$ is a map from $p \in M$ to
$\Lambda^{k} (T_pM)$ by

$$
    \omega(p) = \sum_{i_1 < \cdots < i_k} \omega_{i_1,\cdots, i_k}(p) dx_{i_1}(p) \wedge \cdots \wedge dx_{i_k}(p)
$$

We say $\omega$ is smooth if $\omega_{i_1,\cdots,i_k}(p)$ are smooth with respect to $p$.

And, $0$-form is a smooth function $f: M \rightarrow \R$.

#### Exterior derivative

Exterior derivative $d$ is an operator maps $k$-form to $(k+1)$-form by

$$
    d \omega = \sum_{i_1 < \cdots < i_k} d\omega_{i_1, \cdots i_k}(p) \wedge (dx_{i_1}(p) \wedge \cdots \wedge dx_{i_k}(p))
$$

and for $0$-form $f$,

$$
    df(p) = \sum_{i=1}^{n} \left. \frac{\partial f}{\partial x_i}\right|_{p} dx_{i}(p)
$$

Note that since $dx_i(p)(v) = v_i$,

$$
    df(p)(v) = \sum_{i=1}^{n}  v_i\frac{\partial f}{\partial x_i}(p) = \nabla f(p) \cdot v = df_p(v)
$$

as we defined before.

Note that since $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$ and $ dx_{i} \wedge dx_{j} = -dx_{i} \wedge dx_{j}$, $d(d\omega) = 0$ for every $\omega = 0$.

#### Integration of differential forms

For integrate the differential form on $M$, we need four steps.

Let us define some preliminaries first.

##### Pullback

Let $M, N$ be differentiable manifolds, $\omega$ be a $k$-form on $N$, $f: M \rightarrow N$
be a differentiable map, then $f^{*} \omega$ is $k$-form on $M$ defined by

$$
   f^{*}  \omega(p) (v_1, \cdots, v_k) = \omega(f(p)) (df(p)(v_1), \cdots, df(p)(v_k))
$$

##### Simplicial simplex

Simplicial simplex $\Delta^k \subset \R^{k}$ is a convex hull of
$\{(0, \cdots, 0), e_1, \cdots, e_k\}= \{(t_1, \cdots, t_k) : t_i \geq 0 \forall i, \; \sum_{i} t_i \leq 1\}$

![Simplicial simplex](https://europepmc.org/articles/PMC4430537/bin/pone.0126383.g001.jpg)
*Image from: <https://europepmc.org/article/med/25970184>*

##### Singular simplex and singular chain

Now, let $\sigma: \Delta \rightarrow M$ be a smooth map, then we call $\sigma$ *singular simplex*, and $\sigma = \sum_{i} c_i \sigma_{i}$, the linear combination of singular simplicies, is called *singular chain*.

##### Orientability

##### Integration on euclidean space

First, let us integrate $k$-form on $\R^k$. Let $A$ be a subset of $\R^k$. Then we can write

$$
    \omega = f dx_1 \wedge \cdots \wedge dx_k
$$

and

$$
    \int_{A} \omega  = \int_{A} f dx_1 \cdots dx_k
$$

##### Integration on chain

Now, let $\sigma: \Delta^{k} \rightarrow M$ be a singular simplex, then we define

$$
    \int_{\sigma} \omega = \int_{\Delta^{k}} \sigma^{*} \omega
$$

and if $\sigma = \sum_{i} c_i\sigma_i$ is a singular chain, then

$$
    \int_{\sigma} \omega = \sum_i c_i \int_{\sigma_i} \omega
$$

##### Integration on manifold

### A Quick Introduction to Homology and Cohomology

#### Chain complex

#### Homology

#### Cohomology

##### De rham cohomology

### Discrete Exterior Calculus

## Graph Convolutional Network(GCN)

### Convolution

Let $\mathcal{F}$ be a fourier trasnform operator, and $ \star $ be a convolution operator.
Then

$$\mathcal{F}(f \star g) = \mathcal{F}(f) \mathcal{F}(g)$$

holds.
With this relationship, let $f, g \in \R^{|\mathcal{V}|} $ be a function defined on $\mathcal{V}$.
 we will define 'graph' convolution by

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
$$

for appropriate 'fourier transform' operator for functions on $\mathcal{V}$.
Where $\odot$ represents elementwise multiplication.

### Discrete Fourier Transform and Graph Fourier Transform

### Graph Convolution

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph.
Let $f: \mathcal{V} \rightarrow \mathbb{R}$ be a function defined on vertex set.
Then the fourier transform of $f$ is defined as

$$
    s = U^T f
$$

where $U$ is (normalized) eigenvectors of graph laplacian $L = D - A$.

Then, the graph convolution layer is defined as

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
    = U (U^T f \odot U^T g) = U \textrm{diag} (U^T g) U^T f
$$

## Remarks

1. Graph convolution methods using graph laplacian are called *spectral graph convolution*. Other methods, using message passing framework to construct graph convolution are called *spatial graph convolution*.
2. The *Diffusion Kernel*, which is kernel on feature vector of vertex[2], also can be interpreted with
discrete differential geometry.
The *Diffusion Equation* $  \partial_t f = \mu \Delta f$ with graph laplacian gives the reason why we call it 'Diffusion'.
More specifically, the kernel element $K_{ij}$ can be interpreted as probability of
$i \rightarrow j$ at continuous random walk, and the random walk can be interpreted as
diffusion process on graph along edge.

## References

[1] do Carmo, M. Differential Forms and Applications, Springer, 1994.  
[2] Kondor, R., and Lafferty, J. Diffusion kernels on graphs and other discrete structures. ICML, 2002.  
[3] Lee, J. M. Introduction to Smooth Manifolds, Springer, 2012.  
[4] 김강태, 미분다양체론, 교우사, 2015  
[5] Hamilton, W. L. Graph representation learning, Synthesis Lectures on Artifical Intelligence and Machine Learning 14.3 (2020): 1-159.  
[6] Spivak, M. Calculus on manifolds: a modern approach to classical theorems of advanced calculus. CRC press, 2018.  
[7] McInerney, A. First Steps in Differential Geometry. Springer-Verlag, 2013  
[8] Crane, K. Discrete differential geometry: An applied introduction, Notices of the AMS, Communication (2018): 1153-1159.  
[9] Chen, A. Discrete Differential Geometry, <https://cseweb.ucsd.edu/~alchern/teaching/DDG.pdf>, 2020
