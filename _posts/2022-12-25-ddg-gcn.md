---
title: Discrete Differential Geometry and Graph Convolution
date: 2022-12-25 21:15:00 +0900
categories: [Machine Learning, Graph Neural Network]
tags: [Machine Learning, Graph Neural Network, Differential Geometry, Geometric Deep Learning, Discrete Fourier Transform]
pin: false
math: true
---

## Geometric Deep Learning and Graph Neural Network

Deep Learning is one of the most greatest advance in machine learning.
But, mathematically, neural network is just a parameterized function
$f_{\theta}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and there is no way to
enforce geometric structure of its domain to neural network.

For example, let us assume that our domain is graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
Typically in graph machine learning, there is a node feature vector $X_v \in \mathbb{R}^d$ for each $v \in \mathcal{V}$.

Then we can make a neural network which gets input as $ X = [X_{v_1}, X_{v_2}, \cdots, X_{v_n}] \in \mathbb{R}^{n \times d} $, and adjacency matrix $A$.

Since the ordering of graph does not have meaning, 

$$
    f_{\theta} (X_{v_1}, \cdots, X_{v_n}, A) = f_{\theta} (X_{v_{\sigma(1)}}, \cdots, X_{v_{\sigma(n)}}, PAP^T)
$$

should hold for every permutation $\sigma$ and corresponding permutation matrix $P$ (this property is called permuation invariance).
But there is no way to enforce permutation invariance to $f_{\theta}$.

Geometric Deep Learning is a research field of deep learning, which studies to construct 
neural network with geometric 'symmetry' of domain.
The major domains of geometric deep learning is 
Grids, Groups, Graphs, Geodesics, and Gauges.


Ordinary convolutional layer is a well-known instance of geometric deep learning.
It is 'equivariant' layer of 'Grid'.

For 'Graph', we want to make a permutation invariance layer with the ordering of graph.
The discrete differential geometry can propose an answer to this question, which we call 
(spectral) graph convolution.

## Discrete Differential Geometry


### A Quick Introduction to Differential Geometry
Let us first review basic notions of differential geometry.$\renewcommand{\R}{\mathbb{R}}$

* Smooth Manifold $M$ is a locally euclidean, hausdorff set which every coordinate maps
$
\phi: U\subset M \rightarrow V \subset \R^n, \psi: U'\subset M \rightarrow V' \subset \R^n
$
satisfies if $U \cap U' \neq \emptyset$, then
$$
\phi \circ \psi^{-1} : \psi( U \cap U' )\rightarrow \phi(U \cap U'), \psi \circ \phi^{-1} : \phi(U \cap U') \rightarrow \psi(U\cap U')
$$
are smooth function in $\R^n$.
![manifold](https://ncatlab.org/nlab/files/ChartsOfAManifold.png)
*Image from: [nlab](https://ncatlab.org/nlab/show/differentiable+manifold)*
$f: M \rightarrow \R$ is differential if for each point $p$ and 
corresponding coordinate map $\psi$, $f \circ \psi^{-1}$ is differentiable.


<!-- * Tangent vector $v$ at $p$ is a map from $\mathcal{C}^{\infty}(M)$ to $\R$ such that

    $v(af + bg) = a v(f) + b v(g)$

    $v(fg) = v(f)g(p) + f(p)v(g)$

    Note that this pedantic definition intends to the directional derivative of calculus,
    $ v \cdot \nabla f (p)= D_v f(p) = df (p)(v) $ -->
    
* In calculus and undergraduate differential geometry, we dealt with manifold as an locally euclidean set which embedded in some other euclidean space.
we defined a tangent vector at $p$ by $\alpha'(0)$, where $\alpha: I \rightarrow M \subset \R^n$ is smooth curve and $\alpha(0) = p$.
And the tangent space $T_p M$ is a set of tangent vector at $p$.
If we have a differentiable map $f: \R^m \rightarrow \R^n$, then we defined differential map
$df_p: T_p (\R^n) \rightarrow T_{f(p)} (\R^m)$ as $df_p(v) = (f \circ \alpha)'(0)$, where $\alpha(0) = p, \alpha'(0) = v$.  
Note that in standard basis of euclidean space, $df_p = f'(p)$ and if $m=1$, then $df(p)(v) = \nabla f(p) \cdot v$.  
Let $\newcommand{\x}{\mathbf{x}}\x_p (u, v) = (\x_1(u,v), \x_2(u,v), \x_3(u,v))$ be a local coordinate of surface,
then $\newcommand{\x}{\mathbf{x}}d\x_p(e_i)$ gives standard basis of $T_p M$.
Let $f$ be a differential map on $M$, then we define directional derivative
$ D_v f(p) = \lim_{h \rightarrow 0}\frac{f(p + hv) - f(p)}{h} = \left.\frac{d}{dt}\right|_{t=0} f(p + tv)$,
then with coordinate representation and chain rule, $D_v f(p) = \sum_{i=1}^{n} v_i \frac{\partial f}{\partial x_i}(p) = \nabla f(p) \cdot v$, is same with differential map we defined.  
Now in abstact manifold, we should use local coordinate to use same definitions.


* Vector field $X$ on $M$ is an assignment of tangent vectors $T_p M$ for each $p \in M$.
i.e.
$$
    X: M \rightarrow TM = \bigcup_{p \in M} \{p\} \times T_p M
$$
and we denote 
$$
    X(p) = \sum_{i} v_i (p) \left. \frac{\partial}{\partial x_i} \right|_{p}
$$
where $\left. \frac{\partial}{\partial x_i} \right|_{p}$ are standard basis of $T_p M$.
We say $X$ is smooth if $v_i(p)$ are smooth with respect to $p$.



### A Quick Introduction to Simplicial Complex

### Discrete Exterior Calculus


## Graph Convolution Layer
Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph.
Let $f: \mathcal{V} \rightarrow \mathbb{R}$ be a function defined on vertex set.
Then the fourier transform of $f$ is defined as 

$$
    s = U^T f 
$$

where $U$ is (normalized) eigenvectors of graph laplacian $L = D - A$.

Then, the graph convolution layer is defined as

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g)) 
    = U (U^T f \odot U^T g) = U \textrm{diag} (U^T g) U^T f
$$

Where $\odot$ represents elementwise multiplication.


## Remarks

1. Graph convolution methods using graph laplacian is called *spectral graph convolution*. Other methods, using message passing framework to construct graph convolution is called *spatial graph convolution*.
2. The *Diffusion Kernel*, which is kernel on feature vector of vertex, also can be interpreted with 
discrete differential geometry. 
The *Diffusion Equation* $  \partial_t f = \mu \Delta f$ with graph laplacian gives the reason why we call it 'Diffusion'.
More specifically, the kernel element $K_{ij}$ can be interpreted as probability of
$i \rightarrow j$ at continuous random walk, and the random walk can be interpreted as
diffusion process on graph along edge.


## References