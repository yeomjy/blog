---
title: Discrete Differential Geometry and Graph Convolution
date: 2022-12-27 01:15:00 +0900
categories: [Machine Learning, Graph Neural Network]
tags: [Machine Learning, Graph Neural Network, Differential Geometry, Geometric Deep Learning, Discrete Fourier Transform]
pin: false
math: true
---

## Geometric Deep Learning and Graph Neural Network

Deep Learning is one of the most greatest advance in machine learning.
But, mathematically, neural network is just a parameterized function
$f_{\theta}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and there is no way to
enforce geometric structure of its domain to neural network.

For example, let us assume that our domain is graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
Typically in graph machine learning, there is a node feature vector $X_v \in \mathbb{R}^d$ for each $v \in \mathcal{V}$.

Then we can make a neural network which gets input
$X = \left[X_{v_1}, X_{v_2}, \cdots, X_{v_{|\mathcal{V}|}}\right] \in \mathbb{R}^{|\mathcal{V}| \times d}$, and adjacency matrix $A$.

Since the ordering of graph does not have meaning,

$$
    f_{\theta} \left(X_{v_1}, \cdots, X_{v_{|\mathcal{V}|}}, A\right) = f_{\theta} \left(X_{v_{\sigma(1)}}, \cdots, X_{v_{\sigma(|\mathcal{V}|)}}, PAP^T\right)
$$

should hold for every permutation $\sigma$ and corresponding permutation matrix $P$ (this property is called permuation invariance).
But there is no way to enforce permutation invariance to $f_{\theta}$.

Geometric Deep Learning is a research field of deep learning, which studies to construct
neural network with geometric 'symmetry' of domain.
The major domains of geometric deep learning is
Grids, Groups, Graphs, Geodesics, and Gauges.

Ordinary convolutional layer is a well-known instance of geometric deep learning.
It is 'equivariant' layer of 'Grid'.

For 'Graph', we want to make a permutation invariance layer with the ordering of graph.
The discrete differential geometry can propose an answer to this question, which we call
(spectral) graph convolution.

## Discrete Differential Geometry

### A Quick Introduction to Differential Geometry

Let us first review basic notions of differential geometry.$\renewcommand{\R}{\mathbb{R}}$

#### Topological manifold

Topological manifold $M$ is a topological space such that locally euclidean, hausdorff, second-countable.
The word 'locally euclidean' means that for each $p \in M$, there exists an open neighborhood
$p \in U \subset M$ such that there exists a homeomorphism $\psi: U \rightarrow V$,
where $V$ is an open subset of $\R^n$.
We call the pair $(U, \psi)$ a *coordinate chart*.

##### Manifold with boundary

We also need notion of 'boundary'.
Let $\mathbb{H}^{n} = \{(x_1, \cdots, x_n) \in \R^n : x_n \geq 0\}$ be a half-space.
For $p \in M$, if there exists $\psi: U \rightarrow V \subset \mathbb{H}$ such that
$\psi(p)_n = 0$, then we say $p$ is a boundary point of $M$ and
denote $\partial M$ as all boundary points of $M$.

If $\partial M$ is nonempty, then we say $M$ is **manifold with boundary**, but in this post,
we will not distinguish *manifold* and *manifold with boundary*.

**Remark**

If $M$ is $k$-manifold, then $\partial M$ is $(k-1)$ manifold, and $\partial(\partial M) = \emptyset$.

#### Smooth manifold

Smooth manifold $M$ is a topological manifold which every two coordinate charts
$(U, \psi), (U', \phi)$ satisfies if $U \cap U' \neq \emptyset$, then

$$
\phi \circ \psi^{-1} : \psi( U \cap U' )\rightarrow \phi(U \cap U'), \psi \circ \phi^{-1} : \phi(U \cap U') \rightarrow \psi(U\cap U')
$$

are smooth function on $\R^n$.
![manifold](https://ncatlab.org/nlab/files/ChartsOfAManifold.png)
*Image from: [nlab](https://ncatlab.org/nlab/show/differentiable+manifold)*
Now we can define differentiable map by local coordinate.
$f: M \rightarrow \R^k$ is differentiable if for each $p \in M$, there is a coordinate chart
$(U, \psi)$ containing $p$ such that
$f \circ \psi^{-1}: \psi(U)\subset \R^n \rightarrow \R^k$ is differentiable.
Moreover, if $M, N$ are differentiable manifolds, then
$f: M \rightarrow N$ is differentiable if for each $p \in M$ there are coordinate chart
$(U, \psi)$ containing $p$ and $(V, \varphi)$ containing $f(p)$ such that
$\varphi \circ f \circ \psi^{-1}: \psi(U) \rightarrow \varphi(V)$ is differentiable.

#### Tangent vector and tangent space

In calculus and undergraduate differential geometry, we dealt with manifold as an locally euclidean set which embedded in some other euclidean space.
We defined a **tangent vector** at $p$ by $\alpha'(0)$, where $\alpha: I \rightarrow M$ is a
smooth curve and $\alpha(0) = p$, and the **tangent space** $T_p M$ is a vector space of all tangent vectors at $p$.
And for $f: M \rightarrow N$, we defined a differential of $f$ at $p$, $df_p: T_pM \rightarrow T_{f(p)}N$

$$
    df_p (v) = (f \circ \alpha)'(0)
$$

where $\alpha(0) = p, \alpha'(0) = v$.

Let $f: M \rightarrow \R$, then we defined **directional derivative**

$$
D_v f(p) = \lim_{t \rightarrow 0}\frac{f(p + tv) - f(p)}{t} = \left.\frac{d}{dt}\right|_{t=0} f(p + tv) = \sum_{i=1}^{n} v_i \frac{\partial f}{\partial x_i}(p) = \nabla f(p) \cdot v = df_p(v)
$$  

Now, in abstract manifold setting, we should use local coordinates.
Let $p \in M$, $(U, \psi)$ be a coordinate chart containing $p$, $\alpha: I \rightarrow M$ be a differentiable curve, and $\psi(p) = (x_1(p), \cdots, x_n(p)) = x_0$
Then

$$
D_v f(p) = \left.\frac{d}{dt} f(\alpha(t))\right|_{t=0} = \left.((f\circ \psi^{-1}) \circ (\psi \circ \alpha))\right|_{t=0} = \sum_{i=1}^{n} \left. \frac{\partial (f \circ \psi^{-1})}{\partial x_i} \right|_{x=x_0} \left.\frac{d}{dt} x_i(\alpha(t)) \right|_{t=0}
$$

If $\psi(\alpha(t)) = (x_1, \cdots, x_i + t, \cdots, x_n) $, then we can get **partial derivative**

$$
    \frac{\partial f}{\partial x_i} = \frac{\partial (f \circ \psi^{-1})}{\partial x_i}
$$

Now let us assume that we define **tangent vector** as $(\psi \circ \alpha)'(0)$, then this definition
depends to the coordinate chart $\psi$, so we need independent definition to $\psi$.

Since for each $f, v$ we can define $df_p(v)$, we can think $v$ as a map from $\mathcal{C}^{\infty}(M)$ such that $f \mapsto df_p(v)$.

Therefore, we define **tangent vector** $v_p$ at $p$ as a map

$$ v_p:\mathcal{C}^{\infty}(M) \rightarrow \R $$

$$v_p(af + bg) = a v_p(f) + b v_p(g)$$

$$v_p(fg) = v_p(f)g(p) + f(p)v_p(g)$$

We call it a **derivation**.

Of course, for each $v \in T_p M$, given coordinate chart $(U, \psi)$, there exists $v_1, \cdots, v_n$ such that

$$
    v = \sum_{i=1}^{n} v_i \left.\frac{\partial}{\partial x_i}\right|_{p}
$$

where
$$
\left.\frac{\partial}{\partial x_i}\right|_{p} (f) = \left.\frac{\partial (f \circ \psi^{-1})}{\partial x_i}\right|_{\psi(p)}
$$

#### Vector field

**Vector field** $X$ on $M$ is an assignment of tangent vectors $T_p M$ for each $p \in M$.
i.e.

$$
    X: M \rightarrow TM = \bigcup_{p \in M} \{p\} \times T_p M
$$

and we denote

$$
    X(p) = \sum_{i} v_i (p) \left. \frac{\partial}{\partial x_i} \right|_{p}
$$

where $\left. \frac{\partial}{\partial x_i} \right|_{p}$ are standard basis of $T_p M$.
We say $X$ is smooth if $v_i(p)$ are smooth with respect to $p$.

### A Quick Introduction to Differential Forms

#### Tensor and Tensor product

Let $V_1, \cdots, V_p, W_1, \cdots, W_q$ be vector spaces, then
**$(p, q)$-tensor** $T$ is a multilinear functional

$$
    T: V_1^{*} \times \cdots \times V^{*}_p \times W_1 \times \cdots \times W_q \rightarrow \R
$$

In the context of differential form, we only consider $(0, k)$-tensor (or simply, $k$-tensor)

$$
    T: V\times \cdots \times V \rightarrow \R
$$

and we denote the space of $k$-tensor as $\mathfrak{T}^{k}(V)$.

We can define a **tensor product** $\otimes$ which gets $p$-tensor and $q$-tensor and outputs $(p+q)$-tensor by

$$
    (T \otimes S) (v_1,\cdots, v_p, v_{p+1}, \cdots, v_{p+q}) = T(v_1,\cdots,v_p) S(v_{p+1}, \cdots, v_{p+q})
$$

We can write tensor as a form of basis expansion

$$
    T = \sum_{i_1, \cdots, i_{k}} T_{i_1, \cdots, i_k} dx_{i_1} \otimes \cdots \otimes dx_{i_k}
$$

where $T_{i_1, \cdots, i_k} \in \R$, $dx_i$ are dual basis of $V$.

#### Alternating tensor and Wedge product

**$k$-alternating form** $T$ is a $k$-tensor such that

$$ T(\cdots, v_i, \cdots, v_j, \cdots) = -T(\cdots, v_j , \cdots, v_i, \cdots)$$

and we denote the space of all alternating $k$-tensor as $\Lambda^{k}(V)$.

We also want to write $\omega \in \Lambda^{k}(V)$ as a form of basis expansion.
Therefore, we define **wedge product** $\wedge$ by

$$
    (\phi_1 \wedge \cdots \wedge \phi_k)(v_1, \cdots, v_k) = \det \left( \begin{array}{ccc}
        \phi_1 (v_1)  & \cdots & \phi_1(v_k) \\
        \vdots & \ddots & \vdots \\
        \phi_k (v_k) & \cdots & \phi_k(v_k)
     \end{array} \right)
$$

Where $\phi_i$ are $1$-form.

Then we can write $\omega \in \Lambda^{k}(V)$ by

$$ \omega = \sum_{i_1< \cdots< i_k} \omega_{i_1 , \cdots , i_k} dx_{i_1} \wedge \cdots \wedge dx_{i_k} $$

and if $\omega = \sum_{I} \omega_I dx_I, \varphi = \sum_{J} \varphi_J dx_J$, then

$$
    \omega \wedge \varphi = \sum_{IJ} \omega_I \varphi_J dx_I \wedge dx_J
$$

#### Differential forms

**Differential $k$-form** on manifold $M$ is an assignement of $k$-alternating tensor for each  $p \in M$ by

$$
    \omega(p) = \sum_{i_1 < \cdots < i_k} \omega_{i_1,\cdots, i_k}(p) dx_{i_1}(p) \wedge \cdots \wedge dx_{i_k}(p) \in \Lambda^{k}(T_pM)
$$

We say $\omega$ is smooth if $\omega_{i_1,\cdots,i_k}(p)$ are smooth with respect to $p$.

And, $0$-form is a smooth function $f: M \rightarrow \R$.

#### Exterior derivative

**Exterior derivative** $d$ is an operator maps $k$-form to $(k+1)$-form by

$$
    d \omega = \sum_{i_1 < \cdots < i_k} d\omega_{i_1, \cdots i_k}(p) \wedge (dx_{i_1}(p) \wedge \cdots \wedge dx_{i_k}(p))
$$

and for $0$-form $f$,

$$
    df(p) = \sum_{i=1}^{n} \left. \frac{\partial f}{\partial x_i}\right|_{p} dx_{i}(p)
$$

Note that since $dx_i(p)(v) = v_i$,

$$
    df(p)(v) = \sum_{i=1}^{n}  v_i\frac{\partial f}{\partial x_i}(p) = \nabla f(p) \cdot v = df_p(v)
$$

as we defined before.

Note that since $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$ and $ dx_{i} \wedge dx_{j} = -dx_{i} \wedge dx_{j}$, $d(d\omega) = 0$ for every $\omega = 0$.


### Integration of differential forms

For integrate the differential form on $M$, we need some preparation steps.

Let us define some preliminaries first.

#### Pullback

Let $M, N$ be differentiable manifolds, $\omega$ be a $k$-form on $N$, $f: M \rightarrow N$
be a differentiable map, then $f^{*} \omega$, the **pullback of $\omega$ by $f$**  is $k$-form on $M$ defined by

$$
   f^{*}  \omega(p) (v_1, \cdots, v_k) = \omega(f(p)) (df(p)(v_1), \cdots, df(p)(v_k))
$$

#### Partition of unity

#### Orientability

<!-- Let $M$ be a smooth manifold. -->
To define integration on manifold, we first need the notion of *orientability*.

Because, if the manifold is not orientable, then 

$$
    \int_M \omega = - \int_M \omega
$$

holds and every integral value becomes 0.

First, let us define **orientation of vector space**.

Orientation is defined by equivalence class of ordered basis such that for ordered basis
$[v_1, \cdots, v_n], [u_1, \cdots, u_n]$, let the transtion matrix between basis $P$, then
$[v_1, \cdots, v_n] \sim [u_1, \cdots, u_n]$ if and only if $\det P > 0$.
We choose an orientation $[v_1, \cdots, v_n]$ and say it **positively oriented**.

And we define $[e_1, \cdots, e_n]$ the standard orientation. We typically choose standard orientation
to positive oriented.

Then, for manifold $M$, $M$ is **orientable** if there exists a consistent assignment of orientation for
$T_p M$, with respect to $p$. And we define the collection of such orientations (or assigning function) 
as **orientation of $M$**.

Here is an example of non-orientable manifold, the famous mobuis strip.

![mobuis strip](https://i.stack.imgur.com/WLawj.png)
*Image from: <https://math.stackexchange.com/questions/3266689/do-all-non-orientable-have-to-close-in-some-direction>*

We say orientation $\mathcal{O}$ is positively oriented if for each $p$, orientation of $T_pM$ is
positively oriented.

Furthermore, for $f: M \rightarrow N$, we say $f$ is **orientation-preserving** if
for each $p$, $df_p : T_pM \rightarrow T_{f(p)}N$ maps positive oriented basis to
positive oriented basis.

Finally, we say a coordinate chart $(U, \psi)$ is **positively oriented** if standard basis $\left(\frac{\partial}{\partial x_i}\right)_{i}$

is positively orinted, and a smooth atlas
$\{(U_\alpha, \psi_{\alpha})\}$ is **consistently oriented** if for each $\alpha, \beta$ such that
$U_{\alpha} \cap U_{\beta} \neq \emptyset$, $\psi_{\beta} \circ \psi^{-1}_{\alpha}$ has positive jacobian matrix.

**Remark**

The notion of orientation is quite complicated, but intuitively, orientation is just classifying
the (ordered) basis is counter-clockwise or clockwise (typically, we choose counter-clockwise to positive) and for manifold, orientability is just existence of assigning consistent orientation(typically, counter-clockwise too) for tangent space of each point.

*TODO: orientation of boundary*

#### Simplicial complex

Simplicial complex is a way to represent topological space by vertices.

A **geometric $p$-simplex** $\sigma_p = [v_0, v_1, \cdots, v_p]$ is a convex hull of $\{v_0, \cdots, v_p\}$,
i.e.,

$$
    [v_0, v_1, \cdots, v_p] = \left\{ \sum_{i=0}^{p} t_i v_i : 0\leq t_i \leq 1, \; \sum_{i=0}^{p} t_i = 1 \right\}
$$

and each $v_i$ is called a **vertex** of simplex.

![simplex](/assets/img/ddg-gnn/ss.png)
*Image from: <https://cyphynets.lums.edu.pk/images/HOL-MTNS2006.pdf>*

The **standard $k$-simplex** is

$$
    \Delta^k = [e_0, \cdots, e_{k}] \subset \R^{k}, e_0 = (0, \cdots, 0)
$$

And we define **simplicial complex** is the collection of simplicies of various dimension satisfying

1. If $\sigma \in K$, then every face of $\sigma$ is in $K$
2. The intersection of any two simplices in $K$ is either emtpy or a face of each.

![simplicial complex](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Simplicial_complex_example.svg/300px-Simplicial_complex_example.svg.png)
*Image from: <https://en.wikipedia.org/wiki/Simplicial_complex>*

We define a simplex spanned by a nonempty subset of vertices of $\sigma_p$ a *face* of $\sigma$. i.e.,

$$
    [v_{i_0}, \cdots, v_{i_p}]
$$

for some $i_0, \cdots, i_p$

and we define $i$th **boundary face**

$$
    \partial_i [v_0, \cdots, v_p] = [v_0, \cdots, \hat{v}_i, \cdots, v_p]
$$

Let $K_p$ be a $p$-simplex contained in simplical complex $K$. then we define **chain group**
$C_p(K)$ by $C_p(K)=\{c_1 \sigma_1 + \cdots + c_{|K_p|} \sigma_p : c_i \in \mathbb{Z}\}$, where the sum and multiplication is just a formal sum and formal multiplication. Note that $C_p(K) \simeq \mathbb{Z}^{|K_p|}$.


We also define **orientation** of simplex in similar way, by

$$
    [v_0, \cdots, v_p] = \textrm{sgn}(\sigma)[v_{\sigma(0)}, \cdots, v_{\sigma(p)}]
$$

the **boundary map** $\partial$ is defined by

$$
    \partial [v_0, \cdots, v_p] = \sum_{i} (-1)^{i} \partial_i [v_0, \cdots, v_p] = \sum_{i} (-1)^{i} [v_0, \cdots, \hat{v}_i, \cdots, v_p]
$$

See this image for intuitive illustration, $(-1)^{i}$ coefficient is for natural orientation of boundary.

![Boundary and orientation](/assets/img/ddg-gnn/bdd.png)
*Image from: <https://cyphynets.lums.edu.pk/images/HOL-MTNS2006.pdf>*

Note that $\partial (\partial \sigma) = 0$


#### Singular simplex and singular chain

Now, let $\sigma: \Delta^k \rightarrow M$ be a smooth map, then we call $\sigma$ **singular simplex**, and similarly,

$$\sigma = \sum_{i} c_i \sigma_{i}$$

the linear combination of $k$-singular simplicies, is called **singular chain**.

#### Integration on euclidean space

First, let us integrate $k$-form on $\R^k$. Let $A$ be a subset of $\R^k$. Then we can write

$$
    \omega = f dx_1 \wedge \cdots \wedge dx_k
$$

therefore, we define

$$
    \int_{A} \omega  = \int_{A} f dx_1 \cdots dx_k
$$

#### Integration on chain

Now, let $\sigma: \Delta^{k} \rightarrow M$ be a singular simplex, then we define

$$
    \int_{\sigma} \omega = \int_{\Delta^{k}} \sigma^{*} \omega
$$

and if $\sigma = \sum_{i} c_i\sigma_i$ is a singular chain, then

$$
    \int_{\sigma} \omega = \sum_i c_i \int_{\sigma_i} \omega
$$

#### Integration on manifold

Now it's time to define integration on manifold $M$. First, we will define integration only for *orientable*, compact $k$-manifold $M$, and $\omega$ is $k$-form on $M$.

First, suppose that there exists a singular simplex $\sigma: \Delta^{k} \rightarrow M$ such that
$\textrm{supp }\omega \subset \sigma(\Delta^k)$, then

$$
    \int_{M} \omega = \int_{\sigma} \omega
$$

Otherwise, for atlas $(U_\alpha, \psi_\alpha)$, there exists a partition of unity $\varphi_{i}$,
then

$$
\int_{M} \omega = \sum_{i} \int_{M} \varphi_i \omega
$$

**Remark**

Integration on manifold is quite complicated. But intuitively, it is just a simple process
that divide manifold into locally $\Delta^k$ regions and integrate, and aggregate it with partition of unity.

*TODO: boundary of chain and boundary of manifold*

#### Stokes' theorem

One of the most important theorem in theory of differential forms is the *Stokes' theorem*.

Let $M$ be orientable, compact $k$-dimensional smooth manifold, $\omega$ be a $(k-1)$-form on $M$, then

$$
    \int_{M} d\omega = \int_{\partial M} \omega
$$

holds.

Stokes' theorem gives a good relationship between $d$ and $\partial$ operator.
This relationship becomes more concrete with De Rham cohomology and De Rham's theorem, and in 
discrete differential geometry.

### Laplacian

*TODO*

#### Hodge Star

#### Co-differential

#### Generalization of Laplacian operator

For $f: \R^n \rightarrow \R \in \mathcal{C}(\R^n)$, we defined laplacian operator

$$
    \Delta f = \sum_{i=1}^{n} \frac{\partial^2 f}{\partial x_i^2}
$$

**Laplace-Beltrami operator** 

**Laplace-De Rham operator**

### A Quick Introduction to Homology and Cohomology

*TODO*

#### Chain complex

#### Homology

#### Cohomology

##### De Rham cohomology

##### De Rham's theorem

### Discrete Exterior Calculus

*TODO*

#### Abstract Simplicial Complex

#### Discrete Exterior Calculs

#### Combinatorial Laplacian

## Graph Convolutional Network(GCN)

Now it's time to talk about *Graph Convolution*.

First, we can think node feature vectors

$$
    X = \left[X_{v_1}, X_{v_2}, \cdots, X_{v_{|\mathcal{V}|}}\right] \in \mathbb{R}^{|\mathcal{V}| \times d}
$$

is stack of $d$ functions defined on $\mathcal{V}$.

That is, to think $(X_{v_i})_j = f_j(v_i)$,
for each $f_j: \mathcal{V} \rightarrow \R \simeq \R^{|\mathcal{V}|}$

Then, it is sufficient to define a convolution operator to each $f_j$, because we can stack multiple
convolutional layer to process each *channel* of input, like convolutional layer for image.

### Convolution

Let $\mathcal{F}$ be a fourier trasnform operator, and $ \star $ be a convolution operator.
Then

$$\mathcal{F}(f \star g) = \mathcal{F}(f) \mathcal{F}(g)$$

holds.
With this relationship, let $f, g \in \R^{|\mathcal{V}|} $ be functions defined on $\mathcal{V}$.
 we will define *graph convolution* by

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
$$

for appropriate *fourier transform* operator for functions on $\mathcal{V}$.
Where $\odot$ represents elementwise multiplication.

### Discrete Fourier Transform and Graph Fourier Transform

First, let us define **discrete fourier transform(DFT)**.

Let $(f_0, \cdots f_{d-1}) \in \R^d$ be a discrete signal(discrete function values), then
the discrete fourier transform is

$$
    s_k = \frac{1}{\sqrt{d}} \sum_{t=0}^{d-1} f_t e^{i \frac{2\pi k}{d} t}
$$

It is just a natural discretization of fourier transform. and we say $e^{i \frac{2\pi k}{d} t}$ a fourier basis.

Then, by straightforward calculation, $ \Delta_{t} e^{i \frac{2\pi k}{d} t} = \frac{d^2}{dt^2} e^{i \frac{2\pi k}{d} t} = - \left(\frac{2\pi k}{d}\right)^2e^{i \frac{2\pi k}{d}t}$.

It means that eigenvector of laplacian is fourier basis.

Since we seen that graph laplacian is laplacian operator in the context of discrete exterior calculus,
we define **graph fourier transform** by the same way,

$$
    s = U^T f
$$

where $U = [u_1, \cdots, u_{|\mathcal{V}|}]$ are (normalized) eigenvectors of graph laplacian
$L = D - A$.

### Graph Convolution

Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph.
Let $f: \mathcal{V} \rightarrow \mathbb{R}$ be a function defined on vertex set.
Then the fourier transform of $f$ is defined as

Then, the graph convolution layer is defined as

$$
    f \star g = \mathcal{F}^{-1} (\mathcal{F}(f) \odot \mathcal{F}(g))
    = U (U^T f \odot U^T g) = U \textrm{diag} (U^T g) U^T f
$$

## Remarks

1. Graph convolution methods using graph laplacian are called *spectral graph convolution*. Other methods, using message passing framework to construct graph convolution are called *spatial graph convolution*.
2. The *Diffusion Kernel*, which is kernel on feature vector of vertex[2], also can be interpreted with
discrete differential geometry.
The *Diffusion Equation* $  \partial_t f = \mu \Delta f$ with graph laplacian gives the reason why we call it 'Diffusion'.
More specifically, the kernel element $K_{ij}$ can be interpreted as probability of
$i \rightarrow j$ at continuous random walk, and the random walk can be interpreted as
diffusion process on graph along edge.

## References

[1] Do Carmo, M. P. *Differential Forms and Applications*. Springer Science & Business Media, 1998.  
[2] Kondor, R. I. and Lafferty, J. Diffusion kernels on graphs and other discrete structures. *In proceedings of th 19th international conference on machine learning*, volume 2002, pp. 315-322, 2022.  
[3] Lee, J. M. *Introduction to Smooth Manifolds*, Springer, 2012.  
[4] Hamilton, W. L. Graph representation learning, *Synthesis Lectures on Artifical Intelligence and Machine Learning*, 14(3):1-159, 2020  
[5] Spivak, M. *Calculus on manifolds: a modern approach to classical theorems of advanced calculus*. CRC press, 2018.  
[6] McInerney, A. First Steps in Differential Geometry. Springer-Verlag, 2013  
[7] Crane, K. Discrete differential geometry: An applied introduction, *Notices of the AMS, Communication*, pp.1153-1159, 2018.  
[8] Lee, J. M. *Introduction to topological manifolds*, volume 202. Springer Science & Business Media, 2010.  
[9] Bronstein, M. M., Bruna, J., Cohen, T., and Velickovic, P. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. *arXiv preprint arXiv:2104.13478*, 2021
